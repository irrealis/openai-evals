{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee671a46-8d80-4b2c-91ce-6fcbed129af1",
   "metadata": {},
   "source": [
    "# 230531-1253 gpt-4 evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c892d-6122-4cab-a563-edd055e585ca",
   "metadata": {},
   "source": [
    "## Goal: preliminary calculus evals for GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e287e4-5532-4654-bdb0-087166344a82",
   "metadata": {},
   "source": [
    "Specifics:\n",
    "\n",
    "- Craft a better prompt for the grading model.\n",
    "  - See https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api\n",
    "- Collect calculus problems.\n",
    "  - Work on calculus problems. After each problem, discuss with GPT-4 on ChatGPT-Plus. Take note of problems that are difficult for GPT-4 to solve.\n",
    "  - Transfer such problems, together with incorrect responses, into YAML file.\n",
    "  - For each such problem, print out the student prompt. For each generated solution, print out the grader prompt.\n",
    "  - Send the grader prompt to GPT-4 via ChatGPT-Plus. Questions:\n",
    "    - Does the grader model grade consistently?\n",
    "    - Does the grader model grade correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f36b7eb-5cc9-4ea5-bd07-77a159261bab",
   "metadata": {},
   "source": [
    "## Improving the grader prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd96f228-40c2-4425-bc55-3bf17bc8b69f",
   "metadata": {},
   "source": [
    "Here's the current grader prompt template:\n",
    "\n",
    "```python\n",
    "grader_prompt_template = grader_prompt_template_v1 = '''\n",
    "You are comparing a submitted answer to an expert answer for a given mathematics problem:\n",
    "---\n",
    "[Mathematics Problem]: {input}\n",
    "---\n",
    "[Expert]: {ideal}\n",
    "---\n",
    "[Submission]: {completion}\n",
    "---\n",
    "Use the following rubric to score the submission. Round the score to the nearest integer.\n",
    "\n",
    "[Rubric]:\n",
    "\n",
    "{rubric}\n",
    "---\n",
    "First, write out in a step by step manner your reasoning for the score you gave to be sure that your conclusion is correct. Avoid simply stating the score at the outset. When you are done giving your reasoning, print the score rounded to the nearest integer (without quotes or punctuation) on its own line.\n",
    "\n",
    "Reasoning:\n",
    "'''.strip()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59561b-15fe-46c8-a886-ce19f81647c3",
   "metadata": {},
   "source": [
    "Proposed improvements:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cfd5c-2fac-41b1-96cc-d334a0afeb12",
   "metadata": {},
   "source": [
    "```\n",
    "You are comparing a submitted answer to an expert solution for a given mathematics problem:\n",
    "\n",
    "Mathematics Problem: \"\"\"\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "Submitted Answer: \"\"\"\n",
    "{completion}\n",
    "\"\"\"\n",
    "\n",
    "Expert Solution: \"\"\"\n",
    "{ideal}\n",
    "\"\"\"\n",
    "\n",
    "Use the following rubric to score the submitted answer. First break the answer down step-by-step. Then evaluate each step for correctness and mathematical rigor. Use your step-by-step evaluations to score the answer. Round the score to the nearest integer.\n",
    "\n",
    "Rubric: \"\"\"\n",
    "{rubric}\n",
    "\"\"\"\n",
    "\n",
    "Write out in a step by step manner your reasoning for the score you gave to be sure that your conclusion is correct. Avoid simply stating the score at the outset. When you are done giving your reasoning, print the score in the range 0-100, rounded to the nearest integer (without quotes or punctuation) on its own line.\n",
    "\n",
    "Desired format:\n",
    "Final Answer Correctness: -||-\n",
    "Explanation Correctness: -||-\n",
    "Mathematical Rigor: -||-\n",
    "Correctness of Intermediate Steps: -||-\n",
    "Score:\n",
    "-||-\n",
    "\n",
    "Your evaluation and reasoning:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33728aa5-ffc7-468d-8d0d-898e14663501",
   "metadata": {},
   "source": [
    "Code to read YAML response file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef52a3c6-c545-47d4-a458-afefea25761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_eval import *\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, Math, Latex\n",
    "import openai\n",
    "import ruamel.yaml\n",
    "from ruamel.yaml import YAML\n",
    "from ruamel.yaml.scalarstring import LiteralScalarString\n",
    "from ruamel.yaml.representer import RoundTripRepresenter\n",
    "\n",
    "import datetime\n",
    "import decimal\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9300b9-be8e-4d05-94c8-522ab9da4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sa.create_engine(\"sqlite:///230531-1326_math-evals.db\")\n",
    "session = Session(engine)\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6a65ba-dccb-4e06-915d-ee9106dd449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.drop_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ed9301-def9-43e9-8e4a-05ac86d07b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08aa9857-0f05-4272-982e-6ad841accb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead83a10-3d50-47fe-b2df-8f180d097c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a710abca-e7ff-4c03-b686-c7ea3bc49438",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f80c731-8c8a-4d7c-bbcb-430c93f4dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "yaml=YAML()\n",
    "yaml.default_flow_style = False\n",
    "yaml.allow_unicode = True\n",
    "yaml.encoding = 'utf-8'\n",
    "\n",
    "def represent_decimal(self, data):\n",
    "  value = '0.' if data.is_zero() else str(data.normalize())\n",
    "  return self.represent_scalar(u'tag:yaml.org,2002:float', value)\n",
    "\n",
    "RoundTripRepresenter.add_representer(Decimal, represent_decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cefefaf-8ba5-4c99-8aac-9d432461e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_latex(text):\n",
    "  text = text.replace('\\\\(', '$')\n",
    "  text = text.replace('\\\\)', '$')\n",
    "  text = text.replace('\\\\[', '$$')\n",
    "  text = text.replace('\\\\]', '$$')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31aaa147-f860-4a45-be53-1db3645b2405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Model(id=1, name=GPT-4 via ChatGPT-Plus, notes=, is_example=False)>,\n",
       " <Model(id=2, name=example grader, notes=, is_example=True)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4_chatgpt_plus = Model.get_or_create(session, dict(name='GPT-4 via ChatGPT-Plus'))\n",
    "example_model = Model.get_or_create(session, dict(name='human'), dict(is_example=True))\n",
    "gpt4_chatgpt_plus, example_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e30d1d-6044-4d77-840e-f2a9f0594743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4_ps=<ProblemSet(id=1, name=GPT-4 calculus problem set 230531-1326, notes=)>\n",
      "gpt4_ss=<SubmissionSet(id=1, name=GPT-4 calculus submission set 230531-1326, notes=, is_example=False)>\n",
      "gpt4_es=<EvaluationSet(id=1, name=GPT-4 calculus evaluation set 230531-1326, notes=, is_example=False)>\n",
      "example_es=<EvaluationSet(id=2, name=GPT-4 calculus example-evaluation set 230531-1326, notes=, is_example=True)>\n"
     ]
    }
   ],
   "source": [
    "gpt4_ps = ProblemSet.get_or_create(session,\n",
    "  dict(name='GPT-4 calculus problem set 230531-1326'),\n",
    ")\n",
    "gpt4_ss = SubmissionSet.get_or_create(session,\n",
    "  dict(name='GPT-4 calculus submission set 230531-1326'),\n",
    ")\n",
    "example_ss = SubmissionSet.get_or_create(session,\n",
    "  dict(name='GPT-4 calculus example-submission set 230531-1326'),\n",
    "  dict(is_example=True),\n",
    ")\n",
    "gpt4_es = EvaluationSet.get_or_create(session,\n",
    "  dict(name='GPT-4 calculus evaluation set 230531-1326'),\n",
    ")\n",
    "example_es = EvaluationSet.get_or_create(session,\n",
    "  dict(name='GPT-4 calculus example-evaluation set 230531-1326'),\n",
    "  dict(is_example=True),\n",
    ")\n",
    "print(f'{gpt4_ps=}')\n",
    "print(f'{gpt4_ss=}')\n",
    "print(f'{gpt4_es=}')\n",
    "print(f'{example_es=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad2b887-633d-4449-bdcf-ddeeaebb1b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_ct=1, ss_ct=2, es_ct=2\n"
     ]
    }
   ],
   "source": [
    "ps_ct = session.scalar(sa.select().with_only_columns(sa.func.count(ProblemSet.id)))\n",
    "ss_ct = session.scalar(sa.select().with_only_columns(sa.func.count(SubmissionSet.id)))\n",
    "es_ct = session.scalar(sa.select().with_only_columns(sa.func.count(EvaluationSet.id)))\n",
    "print(f'{ps_ct=}, {ss_ct=}, {es_ct=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "837b0b5a-7123-4a49-b327-6d3bcbe03b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('calculus-responses-gpt4.yaml', 'r') as f:\n",
    "    data = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce8568b4-e57a-4d26-9728-ef769d919548",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problems:\n",
      "- input: |-\n",
      "    If the infinite curve $y=e^{-x}$, $x \\ge 0$, is rotated about the $x$-axis, find the area of the resulting surface.\n",
      "\n",
      "    Explain your reasoning. Use LaTeX with delimiters '\\(' and '\\)' to make mathematical statements. When you are done, repeat your answer by itself on a new line.\n",
      "  ideal: |-\n",
      "    The area of a surface of revolution, when you're rotating the curve \\(y = f(x)\\), \\(a \\leq x \\leq b\\), around the x-axis is given by the formula:\n",
      "\n",
      "    \\[A = 2\\pi \\int_{a}^{b} y \\sqrt{1 + \\left(\\frac{dy}{dx}\\right)^2} dx\\]\n",
      "\n",
      "    This equation comes from summing up the surface areas of infinitesimal frustums (which are approximately cylindrical for small enough changes in \\(x\\)) that make up the surface.\n",
      "\n",
      "    In our case, \\(f(x) = e^{-x}\\), and we're revolving about the x-axis for \\(0 \\leq x < \\infty\\). We need to compute the derivative of \\(f(x)\\), which is \\(f'(x) = -e^{-x}\\).\n",
      "\n",
      "    Plugging these into the formula gives:\n",
      "\n",
      "    \\[A = 2\\pi \\int_{0}^{\\infty} e^{-x} \\sqrt{1 + \\left(-e^{-x}\\right)^2} dx\\]\n",
      "\n",
      "    This integral simplifies to:\n",
      "\n",
      "    \\[A = 2\\pi \\int_{0}^{\\infty} e^{-x} \\sqrt{1 + e^{-2x}} dx\\]\n",
      "\n",
      "    We can compute this using a u-substitution followed by a trigonometric substitution. Let \\(u = e^{-x}\\). Therefore \\(du = -e^{-x} dx\\), \\(u(0)=1\\), and \\(\\lim_{x \\to \\infty}u(x) = 0\\).\n",
      "\n",
      "    Now our integral is\n",
      "\n",
      "    \\[A = -2\\pi \\int_{1}^{0} \\sqrt{1 + u^2} \\du = 2\\pi \\int_{0}^{1} \\sqrt{1 + u^2} du\\]\n",
      "\n",
      "    The presence of \\(\\sqrt{1 + u^2}\\) suggests the trigonometric substitution \\(u = \\tan \\theta\\). Therefore \\(du = \\sec^2(\\theta) d\\theta\\), \\(\\theta = \\tan^{-1} u\\), \\(\\theta(0) = 0\\), \\(\\theta(1) = \\pi/4\\), and \\(\\sqrt{1 + u^2} = \\sqrt{\\sec^2 \\theta} = |\\sec \\theta| = \\sec \\theta\\) on \\((0, \\pi/4)\\).\n",
      "\n",
      "    Now our integral is\n",
      "\n",
      "    \\[A = 2\\pi \\int_{0}^{\\pi/4} \\sec^3 \\theta d\\theta\\]\n",
      "\n",
      "    We can solve the indefinite integral \\(\\int \\sec^3 \\theta d\\theta\\) using integration by parts with \\(s = \\sec \\theta\\) and \\(t = \\sec^2 \\theta d\\theta\\). Therefore \\(ds = \\sec \\theta \\tan \\theta d\\theta\\) and \\(t = \\tan \\theta\\).\n",
      "\n",
      "    The indefinite integral becomes\n",
      "\n",
      "    \\[\\int \\sec^3 \\theta d\\theta = \\sec \\theta \\tan \\theta - \\int \\sec \\theta \\tan^2 \\theta d\\theta\\]\n",
      "\n",
      "    Using the identity \\(\\tan^2 \\theta = \\sec^2 \\theta - 1\\), we can write this as\n",
      "\n",
      "    \\[\\int \\sec^3 \\theta d\\theta = \\sec \\theta \\tan \\theta - \\int \\sec^3 \\theta d\\theta + \\int \\sec \\theta d\\theta\\]\n",
      "\n",
      "    We can now move \\(\\int \\sec^3 d\\theta\\) from the right side to the left and divide by two:\n",
      "\n",
      "    \\[\\int \\sec^3 \\theta d\\theta = \\frac{1}{2} \\left( \\sec \\theta \\tan \\theta + \\int \\sec \\theta d\\theta \\right)\\]\n",
      "\n",
      "    Using \\(\\int \\sec \\theta d\\theta = \\ln |\\sec \\theta + \\tan \\theta| + C\\) we have:\n",
      "\n",
      "    \\[\\int \\sec^3 \\theta d\\theta = \\frac{1}{2} \\left( \\sec \\theta \\tan \\theta + \\ln |\\sec \\theta + \\tan \\theta| + C \\right)\\]\n",
      "\n",
      "    Our original integral becomes\n",
      "\n",
      "    \\[A = \\pi \\left[  \\sec \\theta \\tan \\theta + \\ln |\\sec \\theta + \\tan \\theta| \\right]_{0}^{\\pi/4} \\]\n",
      "\n",
      "    Therefore\n",
      "\n",
      "    \\[A = \\pi [\\sqrt{2} + \\ln(\\sqrt{2} + 1)]\\]\n",
      "\n",
      "    So the area of the surface generated by revolving \\(y=e^{-x}\\) around the x-axis is \\(\\pi [\\sqrt{2} + \\ln(\\sqrt{2} + 1)]\\).\n",
      "  rubric: |-\n",
      "    - Final Answer Correctness (40 points): The final answer provided by the model matches the expert's answer in its simplest form.\n",
      "      - 40: The final answer is mathematically equivalent to the expert's answer and is in its simplest form.\n",
      "      - 0: The final answer is not mathematically equivalent to the expert's answer or is not in its simplest form.\n",
      "    - Explanation Correctness (30 points): The explanation provided by the model to arrive at the answer is correct and logically sound.\n",
      "      - 30: The explanation correctly justifies each step of the problem-solving process.\n",
      "      - 15: The explanation is partially correct but does not correctly justify each step of the problem-solving process.\n",
      "      - 0: The explanation is incorrect or missing.\n",
      "    - Mathematical Rigor (20 points): The submission correctly uses mathematical identities and applies integral and derivative rules.\n",
      "      - 20: The submission demonstrates full mathematical rigor.\n",
      "      - 10: The submission demonstrates some mathematical rigor but makes minor mistakes.\n",
      "      - 0: The submission does not demonstrate mathematical rigor.\n",
      "    - Correctness of Intermediate Steps (10 points): The submission correctly executes each step of the problem-solving process.\n",
      "      - 10: All intermediate steps are correct.\n",
      "      - 5: Some intermediate steps are correct.\n",
      "      - 0: No intermediate steps are correct.\n",
      "  notes: ''\n",
      "  submissions:\n",
      "  - message: |-\n",
      "      The area of a surface of revolution, when you're rotating the curve \\(y = f(x)\\), \\(a \\leq x \\leq b\\), around the x-axis is given by the formula:\n",
      "\n",
      "      \\[A = 2\\pi \\int_{a}^{b} y \\sqrt{1 + \\left(\\frac{dy}{dx}\\right)^2} dx\\]\n",
      "\n",
      "      This equation comes from summing up the surface areas of infinitesimal frustums (which are approximately cylindrical for small enough changes in \\(x\\)) that make up the surface.\n",
      "\n",
      "      In our case, \\(f(x) = e^{-x}\\), and we're revolving about the x-axis for \\(0 \\leq x < \\infty\\). We need to compute the derivative of \\(f(x)\\), which is \\(f'(x) = -e^{-x}\\).\n",
      "\n",
      "      Plugging these into the formula gives:\n",
      "\n",
      "      \\[A = 2\\pi \\int_{0}^{\\infty} e^{-x} \\sqrt{1 + \\left(-e^{-x}\\right)^2} dx\\]\n",
      "\n",
      "      This integral simplifies to:\n",
      "\n",
      "      \\[A = 2\\pi \\int_{0}^{\\infty} e^{-x} \\sqrt{1 + e^{-2x}} dx\\]\n",
      "\n",
      "      We can compute this using a trigonometric substitution. Let \\(e^{-x} = \\sinh{t}\\). Therefore, \\(e^{-2x} = \\sinh^2{t}\\), and \\(dx = -\\coth{t} dt\\).\n",
      "\n",
      "      Now our integral is:\n",
      "\n",
      "      \\[A = 2\\pi \\int_{\\infty}^{0} \\sinh{t} \\sqrt{1 + \\sinh^2{t}} (-\\coth{t}) dt\\]\n",
      "\n",
      "      Simplifying \\(\\sqrt{1 + \\sinh^2{t}}\\) to \\(\\cosh{t}\\) gives:\n",
      "\n",
      "      \\[A = -2\\pi \\int_{\\infty}^{0} \\sinh{t} \\cosh{t} \\coth{t} dt\\]\n",
      "\n",
      "      Using the identity \\(\\sinh{t}\\coth{t} = \\cosh{t}\\), the integral simplifies to:\n",
      "\n",
      "      \\[A = -2\\pi \\int_{\\infty}^{0} \\cosh^2{t} dt\\]\n",
      "\n",
      "      We can evaluate this integral using the identity \\(\\cosh^2{t} = (1/2)(\\cosh{2t} + 1)\\). The integral of \\(\\cosh{2t}\\) over the entire real line is zero (because it's an even function), so the integral simplifies to:\n",
      "\n",
      "      \\[A = -\\pi \\int_{\\infty}^{0} dt\\]\n",
      "\n",
      "      This is a simple integral: the answer is \\(\\pi t\\) evaluated from \\(\\infty\\) to 0, which gives:\n",
      "\n",
      "      \\[A = \\pi \\cdot 0 - \\pi \\cdot \\infty = -\\infty\\]\n",
      "\n",
      "      This suggests that the area of the surface is infinite.\n",
      "\n",
      "      So the area of the surface generated by revolving \\(y=e^{-x}\\) around the x-axis is infinite.\n",
      "    completion: ''\n",
      "    notes: ''\n",
      "    is_example: true\n",
      "    evaluations:\n",
      "    - message: |-\n",
      "        The final answer provided by the model is correct and matches the expert's answer in its simplest form, so the model earns 40 points for Final Answer Correctness. \n",
      "\n",
      "        However, the model's explanation is incorrect in citing the power rule of integration to solve this particular integral. This results in a loss of 30 points for Explanation Correctness. \n",
      "\n",
      "        The model did use the correct formula to arrive at the final answer, but the explanation provided to justify each step lacks mathematical rigor. Therefore, the model earns 10 points for Mathematical Rigor. \n",
      "\n",
      "        Finally, the model did not correctly execute each step of the problem-solving process, as one step involved a mathematical mistake and another step relied on an inappropriate rule. This results in a loss of 0 points for Correctness of Intermediate Steps. \n",
      "\n",
      "        Score: 80\n",
      "      completion: ''\n",
      "      notes: ''\n",
      "      score: 0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yaml.dump(data, sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1b63c-1e15-47de-bc1c-9b78df18ed99",
   "metadata": {},
   "source": [
    "Things to test:\n",
    "\n",
    "- A problem can be part of more than one problem set. Test this as follows: create two YAML files with identical problems but different problem sets. Load both files. After the first load, the problem should be part of the first problem set. The second load should not create any new problems, should not trigger any errors, and afterward the problem should be contained in both problem sets.\n",
    "\n",
    "- If problem ID is specified for a submission, an exception should be raised. This is because the submission should be listed hierarchically under a problem, and that problem's ID should be used when the submission is created. Test this as follows: create a YAML file that specifies a single problem and single submission in hierarchy, in which any problem ID is specified in the submission spec. This should trigger the error.\n",
    "- If a submission is found in the datagbase for the given submission ID, and its problem ID doesn't match that of the parent problem in the hierarchy, an exception should be raised to indicate this inconsistency. This is because the submission should answer a single problem, and the submission's problem shouldn't change. Test this as follows: create a YAML file that specifies two problems, with one problem having a single submission in hierarchy, and the other problem having no submissions. This should create the corresponding problems and submission in the database; the submission's problem should be recorded in the database as the first problem. Use the loaded data hierarchy to generate a second YAML file, which should now record the IDs of the problems and the submission. Modify the second file, moving the submission from the first problem to the second problem in hierarchy. Try to load the modified YAML file. The incorrect placement of the submission should trigger the error.\n",
    "- Similarly, the model ID for a submission shouldn't change.\n",
    "- The submission can, however, be part of more than one submission set. Test this as follows: create two YAML files with identical problems and submissions but different submission sets. Load both files. After the first load, the submission should be part of the first submission set. The second load should not create any new problems or submissions, should not trigger any errors, and afterward the submission should be contained in both submission sets.\n",
    "\n",
    "- If submission ID is specified for an evaluation, an exception should be raised. This is because the evaluation should be listed hierarchically under a submission, and that submission's ID should be used when the evaluation is created. Test this as follows: create a YAML file that specifies a single problem, a single submission, and a single evaluation in hierarchy, in which any submission ID is specified in the evaluation spec. This should trigger the error.\n",
    "- If an evaluation is found in the database for the given evaluation ID, and its submission ID doesn't match that of the parent submission in the hierarchy, an exception should be raised to indicate this inconsistency. This is because the evaluation should evaluation a single submission, and evaluation's submission shouldn't change. Test this as follows: create a YAML file that specifies one problem and two submissions, with one submission having a single evaluation in hierarchy, and the other submission having no evaluations. This should create the corresponding problem, submissions, and evaluation in the database; the evaluation's submissions should be recorded in the database as the first submission. Use the loaded data hierarchy to generate a second YAML file, which should now record the IDs of the problem, the submissions, and the evaluation. Modify the second file, moving the evaluation from the first submission to the second submission in hierarchy. Try to load the modified YAML file. The incorrect placement of the evaluation should trigger the error.\n",
    "- The evaluation can, however, be part of more than one evaluation set. Test this as follows: create two YAML files with identical problems, submissions and evluations but different evaluation sets. Load both files. After the first load, the evaluation should be part of the first evaluation set. The second load should not create any new problems, submissions, or evaluations, should not trigger any errors, and afterward the evaluation should be contained in both evaluations sets.\n",
    "- Similarly, the model ID for an evaluation shouldn't change.\n",
    "\n",
    "This only applies to YAML files. Manually updating these IDs in the database is another matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d74f3cc4-865e-47ef-b0f1-000e5e1d4bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_set = gpt4_ps\n",
    "\n",
    "default_submission_model = gpt4_chatgpt_plus\n",
    "default_example_submission_model = example_model\n",
    "submission_set = gpt4_ss\n",
    "example_submission_set = example_ss\n",
    "\n",
    "default_evaluation_model = gpt4_chatgpt_plus\n",
    "default_example_evaluation_model = example_model\n",
    "evaluation_set = gpt4_es\n",
    "example_evaluation-set = example_es\n",
    "\n",
    "for pn, problem_data in enumerate(data['problems']):\n",
    "  # Load info from YAML.\n",
    "  # Search by either:\n",
    "  # - problem ID, if available, or\n",
    "  # - input if not.\n",
    "  # Place problem into problem set.\n",
    "  # - If problem set is specified, use it.\n",
    "  # - Otherwise, use the default.\n",
    "  problem_id = problem_data.get('problem_id')\n",
    "  input = problem_data['input']    # Required.\n",
    "  ideal = problem_data.get('ideal')\n",
    "  rubric = problem_data.get('rubric')\n",
    "  problem_notes = problem_data.get('rubric')\n",
    "  \n",
    "  # Update or create problem.\n",
    "  problem_search_dict = {}\n",
    "  problem_update_dict = {}\n",
    "  # If problem ID is specified, search by problem ID.\n",
    "  # Otherwise, search by problem input.\n",
    "  if problem_id is not None:\n",
    "    problem_search_dict['id'] = problem_id\n",
    "    problem_update_dict['input'] = input\n",
    "  else:\n",
    "    problem_search_dict['input'] = input\n",
    "  # Update ideal and rubric fields.\n",
    "  if ideal:\n",
    "    problem_update_dict['ideal'] = ideal\n",
    "  if rubric:\n",
    "    problem_update_dict['rubric'] = rubric\n",
    "  problem = Problem.update_or_create(session, problem_search_dict, problem_update_dict)\n",
    "\n",
    "  # Update notes field.\n",
    "  if problem_notes:\n",
    "    problem.notes = problem_notes\n",
    "  \n",
    "  # Place problem into problem set.\n",
    "  # Note: if problem-set ID was specified for problem in YAML, it will be ignored.\n",
    "  problem.problem_sets.append(problem_set)\n",
    "    \n",
    "  for sn, submission_data in enumerate(problem_data.get('submissions')):\n",
    "    # Load info from YAML.\n",
    "    # Search by either:\n",
    "    # - submission ID, if available, or\n",
    "    # - submission message, model ID, and problem ID if not.\n",
    "    #   - If the model ID is specified, use it. Otherwise, use the default.\n",
    "    #     Which default to use depends on whether this is an example.\n",
    "    #     - Note: if this an example submission, and the model is specified\n",
    "    #       but isn't the example model, print a warning.\n",
    "    #   - If the problem ID is specified, use it. Otherwise, use the default.\n",
    "    # Place submission into submission set.\n",
    "    # - If the submission set is specified, use it.\n",
    "    # - Otherwise, use the default.\n",
    "    #   Which default to use depends on whether this is an example.\n",
    "    # - Note: if this is an example submission, and the submission set is\n",
    "    #   specified but isn't an example submission set, print a warning.\n",
    "    submission_id = submission_data.get('submission_id')\n",
    "    submission_completion = submission_data.get('completion')\n",
    "    submission_message = submission_data['message']     # Required.\n",
    "    submission_score = submission_data.get('score')\n",
    "    submission_notes = submission_data.get('notes')\n",
    "    is_example = submission_data.get('is_example')\n",
    "    submission_model_id = submission_data.get('model_id')\n",
    "    # Note: YAML file SHOULD NOT SPECIFY PROBLEM ID because submission specs\n",
    "    # should be hierarchically listed under a problem spec, and that problem's\n",
    "    # ID should be used.\n",
    "    submission_problem_id = submission_data.get('problem_id')\n",
    "    if submission_problem_id is not None:\n",
    "      raise LookupError(f'YAML file should not specify problem_id={submission_problem_id} in submission specs, but does anyway.')\n",
    "    # Note: in the database there's a many-to-many mapping between submissions\n",
    "    # and submission sets. But in the YAML file, at most a single submission set\n",
    "    # is represented.\n",
    "    submission_set_id = submission_data.get('submission_set_id')\n",
    "    # Model ID and problem ID fields can't be null.\n",
    "    # If model ID is specified, use it.\n",
    "    # Otherwise use the default.\n",
    "    # Which default to use depends on whether this is an example submission.\n",
    "    if submission_model_id is not None:\n",
    "      submission_model = Model.get_or_create(session, dict(id=submission_model_id))\n",
    "    else:\n",
    "      if is_example:\n",
    "        submission_model = default_example_submission_model\n",
    "      else:\n",
    "        submission_model = default_submission_model\n",
    "\n",
    "    # Update or create submission.\n",
    "    submission_search_dict = {}\n",
    "    submission_update_dict = {}\n",
    "    # If submissson ID is specified, search by submission ID.\n",
    "    # Otherwise, search by submission message, model, and problem.\n",
    "    if submission_id is not None:\n",
    "      submission_search_dict['id'] = submission_id\n",
    "      submission_update_dict['message'] = submission_message\n",
    "      #submission_update_dict['model'] = submission_model\n",
    "      #submission_update_dict['problem'] = problem\n",
    "    else:\n",
    "      submission_search_dict['message'] = submission_message\n",
    "      submission_search_dict['model'] = submission_model\n",
    "      submission_search_dict['problem'] = problem\n",
    "    # Update is_example field.\n",
    "    if is_example is not None:\n",
    "      submission_update_dict['is_example'] = is_example\n",
    "\n",
    "    submission = Submission.update_or_create(session, submission_search_dict, submission_update_dict)\n",
    "\n",
    "    if submission.problem != problem:\n",
    "      raise RuntimeError(f'submission with {submission.id=} has wrong problem ID {submission.problem_id=} (should be same as {problem.id=}).')\n",
    "    if submission.model != submission_model:\n",
    "      raise RuntimeError(f'submission with {submission.id=} has wrong model ID {submission.model_id=} (should be same as {submission_model.id=}).')\n",
    "\n",
    "    # Update notes and completion fields.\n",
    "    if submission_completion:\n",
    "      submission.completion = submission_completion\n",
    "    if submission_notes:\n",
    "      submission.notes = submission_notes\n",
    "    # Place submission into its submission set.\n",
    "    # Note: if submission-set ID is specified for submission in YAML, it will be ignored.\n",
    "    if is_example:\n",
    "      submission.submission_sets.append(example_submission_set)\n",
    "    else:\n",
    "      submission.submission_sets.append(submission_set)\n",
    "    \n",
    "    for en, evaluation_data in enumerate(submission_data.get('evaluations')):\n",
    "      # Load info from YAML.\n",
    "      # Search by either:\n",
    "      # - evaluation ID, if available, or\n",
    "      # - message, model ID, and submission ID if not.\n",
    "      #   - If the submission ID is specified, use it. Otherwise, use the default.\n",
    "      #   - If the model ID is specified, use it. Otherwise, use the default.\n",
    "      #     Which default to use depends on whether this is for an example submission.\n",
    "      #     - Note: if this is for an example submission, and the model is specified \n",
    "      #       but isn't the example model, print a warning.\n",
    "      # Place evaluation into evaluation set.\n",
    "      # - If the evaluation set is specified, use it.\n",
    "      # - Otherwise, use the default.\n",
    "      #   Which default to use depends on whether this is for an example submission.\n",
    "      #   - Note: if this is for an example submission, and the evaluation set is specified \n",
    "      #     but isn't an example evaluation set, print a warning.\n",
    "      evaluation_id = evaluation_data.get('evaluation_id')\n",
    "      evaluation_completion = evaluation_data.get('completion')\n",
    "      evaluation_message = evaluation_data['message']     # Required.\n",
    "      evaluation_score = evaluation_data.get('score')\n",
    "      evaluation_notes = submission_data.get('notes')\n",
    "      evaluation_model_id = evaluation_data.get('model_id')\n",
    "      # Note: YAML file SHOULD NOT SPECIFY SUBMISSION ID because evaluation specs\n",
    "      # should be hierarchically listed under a submission spec, and that submission's\n",
    "      # ID should be used.\n",
    "      evaluation_submission_id = evaluation_data.get('submission_id')\n",
    "      if evaluation_submission_id is not None:\n",
    "        raise LookupError(f'YAML file should not specify submission_id={evaluation_submission_id} in evaluation specs, but does anyway.')\n",
    "      # Note: in the database there's a many-to-many mapping between evaluations\n",
    "      # and evaluation sets. But in the YAML file, at most a single evaluation set\n",
    "      # is represented.\n",
    "      evaluation_set_id = evaluation_data.get('evaluation_id')\n",
    "      # Model ID and submission ID fields can't be null.\n",
    "      # If model ID is specified, use it.\n",
    "      # Otherwise use the default.\n",
    "      # Which default to use depends on whether this is an example submission.\n",
    "      if evaluation_model_id is not None:\n",
    "        evaluation_model = Model.get_or_create(session, dict(id=evaluation_model_id))\n",
    "      else:\n",
    "        if is_example:\n",
    "          evaluation_model = default_evaluation_model\n",
    "        else:\n",
    "          evaluation_model = default_example_evaluation_model\n",
    "      if evaluation_set_id is not None:\n",
    "        evaluation_set = EvaluationSet.get_or_create(session, dict(id=evaluation_set_id))\n",
    "      else:\n",
    "        if is_example:\n",
    "          evaluation_set = default_evaluation_set\n",
    "        else:\n",
    "          evaluation_set = default_example_evaluation\n",
    "\n",
    "      # Update or create problem.\n",
    "      evaluation_search_dict = {}\n",
    "      evaluation_update_dict = {}\n",
    "      if evaluation_id is not None:\n",
    "        evaluation_search_dict['id'] = evaluation_id\n",
    "        evaluation_update_dict['message'] = evaluation_message\n",
    "        evaluation_update_dict['model'] = evaluation_model\n",
    "        evaluation_update_dict['submission'] = submission\n",
    "      else:\n",
    "        evaluation_search_dict['message'] = evaluation_message\n",
    "        evaluation_search_dict['model'] = evaluation_model\n",
    "        evaluation_search_dict['submission'] = submission\n",
    "      if score is not None:\n",
    "        evaluation_update_dict['score'] = score\n",
    "\n",
    "      evaluation = Evaluation.update_or_create(session, evaluation_search_dict, evaluation_update_dict)\n",
    "        \n",
    "      if evaluation_notes:\n",
    "        evaluation.notes = evaluation_notes\n",
    "      if evaluation_completion:\n",
    "        evaluation.completion = evaluation_completion\n",
    "      evaluation_set.evaluations.append(evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040bd7db-0a4b-42b2-8dbb-9dca7c257952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4c33491-6791-4891-bf5e-3375b8f90349",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1372023594.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    for sn, submission_data in enumerate(problem_data.get('submissions'):\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecb5cf6b-be73-485d-b9fd-ce0dfaeef63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e218656e-299f-47cd-880a-96ac9f34f2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Problem(id=1, notes=- Final Answer Correctness (40 points): The final answer provided by the model matches the expert's answer in its simplest form.\n",
       "   - 40: The final answer is mathematically equivalent to the expert's answer and is in its simplest form.\n",
       "   - 0: The final answer is not mathematically equivalent to the expert's answer or is not in its simplest form.\n",
       " - Explanation Correctness (30 points): The explanation provided by the model to arrive at the answer is correct and logically sound.\n",
       "   - 30: The explanation correctly justifies each step of the problem-solving process.\n",
       "   - 15: The explanation is partially correct but does not correctly justify each step of the problem-solving process.\n",
       "   - 0: The explanation is incorrect or missing.\n",
       " - Mathematical Rigor (20 points): The submission correctly uses mathematical identities and applies integral and derivative rules.\n",
       "   - 20: The submission demonstrates full mathematical rigor.\n",
       "   - 10: The submission demonstrates some mathematical rigor but makes minor mistakes.\n",
       "   - 0: The submission does not demonstrate mathematical rigor.\n",
       " - Correctness of Intermediate Steps (10 points): The submission correctly executes each step of the problem-solving process.\n",
       "   - 10: All intermediate steps are correct.\n",
       "   - 5: Some intermediate steps are correct.\n",
       "   - 0: No intermediate steps are correct.)>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4_ps.problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0878d119-28d3-45dc-a115-755b6cbfa91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row=(1, 2)\n"
     ]
    }
   ],
   "source": [
    "for row in session.execute(sa.text(\"SELECT * FROM problem_set_problem_associations\")):\n",
    "  print(f'{row=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea11a770-44c8-495c-9385-10f06b919cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem_ct=1, submission_ct=1, evaluation_ct=1\n",
      "len(gpt4_ps.problems)=1, len(gpt4_ss.submissions)=1\n",
      "len(gpt4_es.evaluations)=1, len(human_es.evaluations)=0\n"
     ]
    }
   ],
   "source": [
    "problem_ct = session.scalar(sa.select().with_only_columns(sa.func.count(Problem.id)))\n",
    "submission_ct = session.scalar(sa.select().with_only_columns(sa.func.count(Submission.id)))\n",
    "evaluation_ct = session.scalar(sa.select().with_only_columns(sa.func.count(Evaluation.id)))\n",
    "print(f'{problem_ct=}, {submission_ct=}, {evaluation_ct=}')\n",
    "print(f'{len(gpt4_ps.problems)=}, {len(gpt4_ss.submissions)=}')\n",
    "print(f'{len(gpt4_es.evaluations)=}, {len(example_es.evaluations)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399657ec-3884-4315-9402-f3b7f1320c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pn, problem_data in enumerate(problems_data):\n",
    "  problem = Problem.update_or_create(session,\n",
    "    search_dict=dict(\n",
    "      input=problem_data['input'],\n",
    "      problem_set_id=integration_table_ps.id\n",
    "    ),\n",
    "    update_dict=dict(\n",
    "      ideal=problem_data['ideal'],\n",
    "      rubric=problem_data['rubric']\n",
    "    ),\n",
    "  )\n",
    "  for sn, submission_data in enumerate(problem_data.get('completions', [])):\n",
    "    message = submission_data['completion']\n",
    "    submission = Submission.update_or_create(session,\n",
    "      search_dict=dict(\n",
    "        problem_id=problem.id,\n",
    "        submission_set_id=integration_table_ss.id,\n",
    "        message=message,\n",
    "      ),\n",
    "      update_dict=dict(\n",
    "        notes_json='\"\"'\n",
    "      ),\n",
    "    )\n",
    "\n",
    "    notes_json = json.dumps(\n",
    "      dict(example_evaluations = [\n",
    "        dict(\n",
    "          evaluator = 'KN',\n",
    "          comment = submission_data['comment'],\n",
    "          correct = submission_data['correct'],\n",
    "        ),\n",
    "      ])\n",
    "    )\n",
    "    example_evaluation = Evaluation.update_or_create(session,\n",
    "      search_dict=dict(\n",
    "        submission_id=submission.id,\n",
    "        evaluation_set_id=integration_table_example_es.id,\n",
    "        message=submission_data['comment'],\n",
    "      ),\n",
    "      update_dict=dict(\n",
    "        notes_json=notes_json\n",
    "      ),\n",
    "    )\n",
    "\n",
    "    for en, evaluation_data in enumerate(submission_data.get('evaluations', [])):\n",
    "      evaluation = Evaluation.get_or_create(session,\n",
    "        search_dict=dict(\n",
    "          submission_id=submission.id,\n",
    "          evaluation_set_id=integration_table_es.id,\n",
    "          message=evaluation_data['evaluation']\n",
    "        ),\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419caa1-2239-44d5-8a07-327f16928016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
